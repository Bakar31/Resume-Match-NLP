{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# required libraries\r\n",
    "import os\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "from pdfminer import high_level\r\n",
    "\r\n",
    "#paths\r\n",
    "train_path = \"dataset/trainResumes/\"\r\n",
    "test_path = \"dataset/testResumes/\"\r\n",
    "\r\n",
    "# epty list for resumes text\r\n",
    "train_resumes = []\r\n",
    "test_resumes = []\r\n",
    "\r\n",
    "# pdf2string\r\n",
    "def pdf2string(path, resumes):\r\n",
    "    for i in os.listdir(path):\r\n",
    "        main_path = path+i\r\n",
    "        text = high_level.extract_text(main_path)\r\n",
    "        str_list = text.split()\r\n",
    "        str_list = str_list[:]\r\n",
    "        string = ' '.join(str_list)\r\n",
    "        resumes.append(string)\r\n",
    "\r\n",
    "pdf2string(train_path, train_resumes)\r\n",
    "pdf2string(test_path, test_resumes)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import spacy\r\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\r\n",
    "nlp = spacy.load('en_core_web_sm')\r\n",
    "\r\n",
    "needless_words = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U',\r\n",
    "'V', 'W', 'X', 'Y', 'Z']\r\n",
    "punctuations = list('''!()-[]{};:'\"\\,<>./?@#$%^&*_~''')\r\n",
    "\r\n",
    "def text_processing(resume):   \r\n",
    "\r\n",
    "    resume = nlp(resume)\r\n",
    "    token_list = []\r\n",
    "    for token in resume:\r\n",
    "        token_list.append(token.text)\r\n",
    "\r\n",
    "    filtered_sentence =[] \r\n",
    "    for word in token_list:\r\n",
    "        lexeme = nlp.vocab[word]\r\n",
    "        if lexeme.is_stop == False:\r\n",
    "            filtered_sentence.append(word) \r\n",
    "\r\n",
    "    # further filter\r\n",
    "    filtered_sentence_2 = []\r\n",
    "    for word in filtered_sentence:\r\n",
    "        if word not in needless_words:\r\n",
    "            filtered_sentence_2.append(word)\r\n",
    "\r\n",
    "    filtered_sentence_3 = []\r\n",
    "    for word in filtered_sentence_2:\r\n",
    "        if word not in punctuations:\r\n",
    "            filtered_sentence_3.append(word)\r\n",
    "    \r\n",
    "    Stem_words = []\r\n",
    "    sentence = ' '.join(filtered_sentence_3)\r\n",
    "    doc = nlp(sentence)\r\n",
    "    for word in doc:\r\n",
    "        Stem_words.append(word.lemma_)\r\n",
    "\r\n",
    "    main_text = ' '.join(Stem_words)\r\n",
    "    main_text = main_text.lower()\r\n",
    "    return main_text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "processed_resumes_train = []\r\n",
    "processed_resumes_test = []\r\n",
    "\r\n",
    "for  resume in train_resumes:\r\n",
    "    processed_resume = text_processing(resume)\r\n",
    "    processed_resumes_train.append(processed_resume)\r\n",
    "\r\n",
    "for  resume in test_resumes:\r\n",
    "    processed_resume = text_processing(resume)\r\n",
    "    processed_resumes_test.append(processed_resume)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "print(len(processed_resumes_train))\r\n",
    "print(len(processed_resumes_test))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "90\n",
      "60\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "print(processed_resumes_train[2])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "associate analyst skills certified data analyst degree electronics engineering hand experience analyze interpret datum good numerical accuracy python machine learning mysql data mining deep learning data analysis computer vision flask api predictive modeling aws scikit learn numpy statistical analysis multivariate analysis decision trees random forest xgboost nlp project work experience deep learning base pattern match auto color grade python amz loans mortgages erc analytics jun 2019 till date qualification google cloud certified handling datum employee find retention factor employee satisfaction worked closely hr team find balance beneficial employee company education b.tech b.e. electronics telecommunication nagpur university 2019\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "train = pd.read_csv('dataset/train.csv')\r\n",
    "test = pd.read_csv('dataset/test.csv')\r\n",
    "\r\n",
    "def dataframe(resume_list, df):\r\n",
    "    resumes =  pd.DataFrame(resume_list, columns = ['resumes'])\r\n",
    "    dataframe = pd.concat([df, resumes], axis = 1)\r\n",
    "    dataframe.drop('CandidateID', axis = 1, inplace = True)\r\n",
    "    return dataframe\r\n",
    "\r\n",
    "train_df = dataframe(processed_resumes_train, train)\r\n",
    "test_df = dataframe(processed_resumes_test, test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "train_df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Match Percentage</th>\n",
       "      <th>resumes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13.60</td>\n",
       "      <td>jacob smith personal profile work background a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>36.63</td>\n",
       "      <td>brianna williams executive profile work experi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54.93</td>\n",
       "      <td>associate analyst skills certified data analys...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41.46</td>\n",
       "      <td>python machine learn deep learning data analys...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>48.91</td>\n",
       "      <td>jennifer armstrong fresher computer vision mac...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Match Percentage                                            resumes\n",
       "0             13.60  jacob smith personal profile work background a...\n",
       "1             36.63  brianna williams executive profile work experi...\n",
       "2             54.93  associate analyst skills certified data analys...\n",
       "3             41.46  python machine learn deep learning data analys...\n",
       "4             48.91  jennifer armstrong fresher computer vision mac..."
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "filtered_resume_list_train = list(train_df.resumes)\r\n",
    "filtered_resume_list_test = list(test_df.resumes)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "vocab_size = 1000\r\n",
    "embedding_dim = 16\r\n",
    "max_length = 100\r\n",
    "trunc_type='post'\r\n",
    "padding_type='post'\r\n",
    "oov_tok = \"<OOV>\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "import tensorflow as tf\r\n",
    "import numpy as np\r\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\r\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "#tokenizing sentences\r\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\r\n",
    "tokenizer.fit_on_texts(filtered_resume_list_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "word_index = tokenizer.word_index\r\n",
    "total_words = len(word_index)+1 #1 for oov word\r\n",
    "print(total_words)\r\n",
    "print(word_index)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1897\n",
      "{'<OOV>': 1, 'data': 2, 'learning': 3, 'machine': 4, 'analysis': 5, 'python': 6, '2019': 7, 'work': 8, 'datum': 9, '2020': 10, 'learn': 11, 'b': 12, 'tech': 13, 'project': 14, 'business': 15, 'science': 16, 'experience': 17, 'deep': 18, 'analytics': 19, 'model': 20, 'university': 21, 'analyst': 22, 'engineer': 23, 'base': 24, 'processing': 25, 'profile': 26, 'software': 27, 'skills': 28, 'language': 29, 'education': 30, 'natural': 31, 'intern': 32, 'college': 33, 'system': 34, 'skill': 35, 'developer': 36, 'development': 37, 'engineering': 38, 'till': 39, 'date': 40, 'intelligence': 41, 'management': 42, 'sql': 43, 'nlp': 44, 'computer': 45, 'text': 46, 'solution': 47, 'problem': 48, '2018': 49, 'history': 50, 'mining': 51, 'regression': 52, 'statistical': 53, 'visualization': 54, 'personal': 55, 'cloud': 56, 'executive': 57, 'fresher': 58, 'professional': 59, 'artificial': 60, 'create': 61, 'tableau': 62, 'analytic': 63, 'different': 64, 'technology': 65, 'application': 66, 'time': 67, 'junior': 68, 'look': 69, 'extra': 70, 'scientist': 71, 'big': 72, 'aws': 73, 'end': 74, 'develop': 75, 'modeling': 76, 'image': 77, 'like': 78, 'vision': 79, 'predictive': 80, '2021': 81, 'understand': 82, 'research': 83, 'team': 84, 'ongoing': 85, 'jan': 86, 'algorithm': 87, 'curriculars': 88, 'role': 89, 'projects': 90, 'customer': 91, 'proficient': 92, 'background': 93, 'recognition': 94, 'neural': 95, 'java': 96, 'currently': 97, 'building': 98, 'opportunity': 99, 'decision': 100, 'classification': 101, 'activity': 102, 'process': 103, 'prediction': 104, 'employment': 105, 'educational': 106, 'client': 107, 'risk': 108, 'numpy': 109, 'solve': 110, 'associate': 111, 'tensorflow': 112, 'apr': 113, 'summary': 114, 'academic': 115, 'm': 116, 'nov': 117, 'ml': 118, 'dec': 119, 'drive': 120, 'analytical': 121, 'excel': 122, 'knowledge': 123, 'analyze': 124, 'c': 125, 'ltd': 126, 'network': 127, 'production': 128, 'detection': 129, 'requirement': 130, 'multiple': 131, 'design': 132, 'pandas': 133, 'insight': 134, 'networks': 135, 'jun': 136, 'company': 137, 'pipeline': 138, 'feb': 139, 'oct': 140, 'azure': 141, 'large': 142, 'perform': 143, 'strong': 144, 'institute': 145, 'database': 146, 'programming': 147, 'relate': 148, 'linear': 149, 'electronics': 150, 'scikit': 151, 'pattern': 152, 'graduate': 153, 'series': 154, 'achievement': 155, 'quality': 156, 'service': 157, 'advanced': 158, 'keras': 159, 'build': 160, 'testing': 161, 'matplotlib': 162, 'extract': 163, 'chatbot': 164, 'good': 165, 'flask': 166, 'qualification': 167, 'worked': 168, 'solutions': 169, 'tool': 170, 'activities': 171, 'domain': 172, 'technique': 173, 'computers': 174, 'help': 175, 'sc': 176, 'extraction': 177, 'algorithms': 178, 'information': 179, 'ai': 180, 'pytorch': 181, 'pyspark': 182, 'market': 183, 'seek': 184, 'mysql': 185, 'power': 186, 'bi': 187, 'find': 188, 'contribute': 189, 'level': 190, 'telecommunication': 191, 'pvt': 192, 'iit': 193, 'year': 194, 'predict': 195, 'trainee': 196, 'statistics': 197, 'recommender': 198, 'want': 199, 'explore': 200, '2017': 201, 'identify': 202, 'provide': 203, 'objective': 204, 'clustering': 205, 'object': 206, 'face': 207, 'logistic': 208, 'certificate': 209, 'degree': 210, 'e': 211, 'communication': 212, 'device': 213, 'understanding': 214, 'bert': 215, 'modelling': 216, 'collect': 217, 'assistant': 218, 'sep': 219, 'gathering': 220, 'product': 221, 'individual': 222, 'improve': 223, 'rnn': 224, 'working': 225, 'jul': 226, 'report': 227, 'real': 228, 'method': 229, 'opencv': 230, 'deployment': 231, 'powerbi': 232, 'result': 233, 'sas': 234, 'new': 235, 'expert': 236, 'smart': 237, 'hive': 238, 'ca': 239, 'msc': 240, 'train': 241, 'hadoop': 242, 'google': 243, 'employee': 244, 'satisfaction': 245, 'well': 246, 'dashboard': 247, 'introduction': 248, 'certification': 249, 'collection': 250, 'environment': 251, 'structures': 252, 'tata': 253, 'services': 254, 'dataset': 255, 'search': 256, 'approach': 257, 'determine': 258, 'architecture': 259, 'sentiment': 260, 'automation': 261, 'structure': 262, 'automate': 263, 'bank': 264, 'credit': 265, 'delivery': 266, 'spark': 267, 'pune': 268, 'focus': 269, 'evaluation': 270, 'sde': 271, 'implement': 272, 'use': 273, 'order': 274, 'source': 275, 'field': 276, 'strategy': 277, 'ece': 278, 'kind': 279, 'kerala': 280, 'enhance': 281, 'api': 282, 'sap': 283, 'come': 284, 'speech': 285, 'edge': 286, 'task': 287, 'user': 288, 'make': 289, 'django': 290, 'consultancy': 291, 'firebase': 292, 'join': 293, 'apply': 294, 'optimization': 295, 'html': 296, 'vendor': 297, 'easy': 298, 'feature': 299, 'india': 300, 'fraud': 301, 'singh': 302, 'delhi': 303, 'complex': 304, 'mar': 305, 'line': 306, 'handle': 307, 'expertise': 308, 'sikkim': 309, 'identification': 310, 'forecasting': 311, 'produce': 312, 'linux': 313, 'financial': 314, 'noida': 315, 'detail': 316, 'internship': 317, 'elastic': 318, 'support': 319, 'curricular': 320, 'chemical': 321, 'synthesis': 322, 'impact': 323, 'vit': 324, 'aug': 325, 'bsc': 326, 'leverage': 327, 'meaningful': 328, 'organization': 329, 'a': 330, '1': 331, 'hand': 332, 'accuracy': 333, 'grade': 334, 'factor': 335, 'nagpur': 336, 'involve': 337, 't': 338, 'runner': 339, 'require': 340, 'investment': 341, 'exploration': 342, 'anomaly': 343, 'warehouse': 344, 'ready': 345, 'cleaning': 346, 'determination': 347, 'electrical': 348, 'kubernetes': 349, 'start': 350, 'sp': 351, 'server': 352, 'infrastructure': 353, '2': 354, 'hands': 355, 'award': 356, 'lstm': 357, 'website': 358, 'analyzer': 359, 'advance': 360, 'industry': 361, 'assist': 362, 'include': 363, 'st': 364, 'way': 365, 'deploy': 366, 'certify': 367, 'manipulation': 368, 'economics': 369, 'set': 370, 'specialist': 371, 'keen': 372, 'statistic': 373, 'embedded': 374, 'passionate': 375, 'flow': 376, 'it': 377, 'bca': 378, 'key': 379, 'attention': 380, 'skilled': 381, 'core': 382, 'systems': 383, 'acquaint': 384, 'nltk': 385, 'ocr': 386, 'mongodb': 387, 'marketing': 388, 'give': 389, 'technical': 390, 'docker': 391, 'control': 392, 'test': 393, 'dbms': 394, 'commercial': 395, 'target': 396, 'ms': 397, 'amity': 398, 'd': 399, 'integration': 400, 'transaction': 401, 'ap': 402, 'ec2': 403, 'music': 404, 'take': 405, 'input': 406, 'internal': 407, 'inform': 408, 'challenge': 409, 'growth': 410, 'component': 411, 'certified': 412, 'trees': 413, 'random': 414, 'forest': 415, 'retention': 416, 'closely': 417, 'motivate': 418, 'grow': 419, 'ability': 420, 'innovative': 421, 'web': 422, 'job': 423, 'manage': 424, 'corporate': 425, 'interested': 426, 'international': 427, 'kolkata': 428, 'mba': 429, 'utilize': 430, 'specialization': 431, 'transfer': 432, 'framework': 433, 'outcome': 434, 'online': 435, 'bangalore': 436, 'intelligent': 437, 'transportation': 438, 'civil': 439, 'engine': 440, 'mechanism': 441, 'right': 442, 'extensive': 443, 'course': 444, 'lucknow': 445, 'parameter': 446, 'tune': 447, 'pre': 448, 'sai': 449, 'category': 450, 'bba': 451, 'future': 452, 'conversational': 453, 'computing': 454, 'have': 455, 'bot': 456, 'easily': 457, 'josephs': 458, 'arts': 459, 'bengaluru': 460, 'academy': 461, 'hyderabad': 462, 'figure': 463, 'validation': 464, 'analyzing': 465, 'finance': 466, 'reporting': 467, 'home': 468, 'limited': 469, 'state': 470, 'art': 471, 'area': 472, 'orient': 473, 'interest': 474, 'kafka': 475, 'previously': 476, 'sons': 477, 'code': 478, 'view': 479, 'small': 480, 'expand': 481, 'javascript': 482, 'group': 483, 'final': 484, 'study': 485, 'medical': 486, 'program': 487, 'world': 488, 'seaborn': 489, 'achieve': 490, 'concept': 491, 'chain': 492, 'visualisation': 493, 'labs': 494, 'rajiv': 495, 'gandhi': 496, 'unsupervised': 497, 'nosql': 498, 'retail': 499, 'ibm': 500, 'cse': 501, 'knn': 502, 'offer': 503, 'training': 504, 'matlab': 505, 'milestone': 506, 'microsoft': 507, 'track': 508, 'communicate': 509, 'debug': 510, 'methodology': 511, 'dashboards': 512, 'eda': 513, 'fit': 514, 'rdbms': 515, 'box': 516, 'performance': 517, 'high': 518, 'manager': 519, 'optimize': 520, 'air': 521, 'word': 522, 'document': 523, 'enthusiast': 524, 'transform': 525, 'amazon': 526, 'store': 527, 'portfolio': 528, 'return': 529, 'consultant': 530, 'compound': 531, 'improvement': 532, 'cdn': 533, 'activite': 534, 'actively': 535, 'lead': 536, 'success': 537, 'hub': 538, 'parking': 539, 'genre': 540, 'eye': 541, 'smile': 542, 'masters': 543, 'inc': 544, 'contributed': 545, 'mamco': 546, 'thinking': 547, 'continuous': 548, 'correlation': 549, 'spss': 550, 'response': 551, 'interpret': 552, 'multivariate': 553, 'xgboost': 554, 'auto': 555, 'amz': 556, 'loans': 557, 'mortgages': 558, 'erc': 559, 'handling': 560, 'balance': 561, 'maintain': 562, 'practice': 563, 'request': 564, 'allocation': 565, 'estimation': 566, 'react': 567, 'app': 568, 'v': 569, 'solinovate': 570, 'bits': 571, 'goa': 572, 'technologies': 573, 'address': 574, 'portal': 575, 'medalist': 576, 'member': 577, 'student': 578, 'innovation': 579, 'rich': 580, 'irrm': 581, 'remote': 582, 'expenditure': 583, 'school': 584, 'datacamp': 585, 'intermediate': 586, 'discovery': 587, 'familiar': 588, 'transition': 589, 'computation': 590, 'librarie': 591, 'chat': 592, 'coursework': 593, 'medium': 594, 'comment': 595, 'journey': 596, 'segmentation': 597, 'life': 598, 'position': 599, 'location': 600, 'nanyang': 601, 'technological': 602, 'oracle': 603, 'preprocesse': 604, 'vehicle': 605, 'gcp': 606, 'crash': 607, 'payment': 608, 'best': 609, 'performer': 610, 'strategic': 611, 'exchange': 612, 'challenging': 613, 'transformers': 614, 'mahindra': 615, 'ew': 616, 'post': 617, 'ticket': 618, 'issue': 619, 'transformer': 620, 'range': 621, 'responsibility': 622, 'select': 623, 'overall': 624, 'venture': 625, 'wrangling': 626, 'dataflow': 627, 'vellore': 628, 'firm': 629, 'scale': 630, 'postgresql': 631, 'loan': 632, 'classifier': 633, 'practitioner': 634, 'svm': 635, 'indian': 636, 'contain': 637, 'behavior': 638, 'link': 639, 'economic': 640, 'radio': 641, 'office': 642, 'embed': 643, 'connect': 644, 'arduino': 645, 'l': 646, 'highly': 647, 'mentality': 648, 'mangalore': 649, 'fine': 650, 'part': 651, 'assembly': 652, 'scalable': 653, 'php': 654, 'teaching': 655, 'agro': 656, 'mca': 657, 'able': 658, 'aspect': 659, 'ruby': 660, 'enterprises': 661, 'stage': 662, 'detect': 663, 'workshop': 664, 'digital': 665, 'learner': 666, 'informatics': 667, 'price': 668, 'engagement': 669, 'module': 670, 'clean': 671, 'excellent': 672, 'debugging': 673, 'images': 674, 'foundation': 675, 'child': 676, 'need': 677, 'manipal': 678, 'love': 679, 'xlnet': 680, 'gpt': 681, 'convert': 682, 'voice': 683, 'memorial': 684, 'identity': 685, 'increase': 686, 'mean': 687, 'national': 688, 'srm': 689, 'green': 690, 'sales': 691, 'index': 692, 'comparison': 693, 'insurance': 694, 'raja': 695, 'ranchi': 696, 'infotech': 697, 'creating': 698, 'deeplearning': 699, 'ann': 700, 'platform': 701, 'demand': 702, 'punjab': 703, 'models': 704, 'researcher': 705, 'kharagpur': 706, 'performed': 707, 'duration': 708, 'sla': 709, 'generation': 710, 'iiit': 711, 'availability': 712, 'troubleshooting': 713, 'instrumentation': 714, 'agile': 715, 'email': 716, 'switch': 717, 'selenium': 718, 'jenkins': 719, 'tuning': 720, 'question': 721, 'gpu': 722, 'monitor': 723, 'sensor': 724, 'house': 725, 'git': 726, 'type': 727, 'kvoct': 728, 'non': 729, 'likelihood': 730, 'purchase': 731, 'beginners': 732, 'leadership': 733, 'display': 734, 'evaluate': 735, 'metric': 736, 'volume': 737, 'decide': 738, 'scenario': 739, 'movie': 740, 'publicis': 741, 'sapient': 742, 'disease': 743, 'jeremy': 744, 'case': 745, 'unstructured': 746, 'assessment': 747, 'file': 748, 'mini': 749, 'storage': 750, 'complete': 751, 'young': 752, 'lexicon': 753, 'semantic': 754, 'orientation': 755, 'db': 756, 'adapt': 757, 'descriptive': 758, 'organize': 759, 'disseminate': 760, 'significant': 761, 'amount': 762, 'virtual': 763, 'ratio': 764, 'adept': 765, 'transformation': 766, 'deliver': 767, 'important': 768, 'maulana': 769, 'graphql': 770, 'believe': 771, 'potential': 772, 'dive': 773, 'script': 774, 'managing': 775, 'proudyogiki': 776, 'vishwavidyalaya': 777, 'rgpv': 778, 'bhopal': 779, 'hansraj': 780, 'beautiful': 781, 'soup': 782, 'try': 783, 'define': 784, 'ksv': 785, 'kochi': 786, 'developed': 787, 'serve': 788, 'presales': 789, 'rfp': 790, 'discipline': 791, 'travel': 792, 'managerial': 793, 'master': 794, 'commerce': 795, 'blockchain': 796, 'to': 797, 'apache': 798, 'charan': 799, 'ludhiana': 800, 'css': 801, 'bgbs': 802, 'mohantpur': 803, 'character': 804, 'audience': 805, 'career': 806, 'springboot': 807, 'ui': 808, 'reinforcement': 809, 'jacob': 810, 'smith': 811, 'goal': 812, 'discover': 813, 'emr': 814, 's3': 815, 'hql': 816, 'brianna': 817, 'williams': 818, 'kpmmc': 819, '5': 820, 'questions': 821, 'answers': 822, 'similar': 823, 'curiosity': 824, 'eager': 825, 'ensure': 826, 'logical': 827, 'consistent': 828, 'teamwork': 829, 'cluster': 830, 'principal': 831, 'anova': 832, 'tree': 833, 'feedback': 834, 'mirror': 835, 'jewelry': 836, 'numerical': 837, 'match': 838, 'color': 839, 'hr': 840, 'beneficial': 841, 'extremely': 842, 'constantly': 843, 'professionally': 844, 'confident': 845, 'iot': 846, 'automatically': 847, 'turn': 848, 'tone': 849, 'mqtt': 850, 'architectural': 851, 'facilitate': 852, 'i': 853, 'upcoming': 854, 'acs': 855, 'ongoingacs': 856, 'jennifer': 857, 'armstrong': 858, 'fun': 859, 'term': 860, 'contextual': 861, 'reasoning': 862, 'guwahati': 863, 'gan': 864, 'modern': 865, 'fashion': 866, 'attendance': 867, 'distinction': 868, 'honorary': 869, 'council': 870, 'innovator': 871, 'conjugation': 872, 'isabella': 873, 'reed': 874, 'computationally': 875, 'loves': 876, 'proper': 877, 'rcc': 878, 'praxis': 879, 'attrition': 880, 'divvy': 881, 'rider': 882, 'cardiac': 883, 'arrest': 884, 'mia': 885, 'park': 886, 'sequential': 887, 'hotspot': 888, 'adversarial': 889, 'patch': 890, 'attack': 891, 'wtpl': 892, 'filtration': 893, 'goverment': 894, 'ceramic': 895, 'methodologies': 896, 'player': 897, 'criticism': 898, 'ep': 899, 'polarisation': 900, 'toxicity': 901, 'iabac': 902, 'cutthroat': 903, 'outlet': 904, 'people': 905, 'presence': 906, 'pgg': 907, 'conway': 908, 'game': 909, 'symmetrical': 910, 'shri': 911, 'guru': 912, 'gobind': 913, 'singhji': 914, 'naned': 915, 'unitanalyze': 916, 'operational': 917, 'derive': 918, 'customersbase': 919, 'positioning': 920, 'coarse': 921, 'grain': 922, 'locationdata': 923, 'dictis': 924, 'responsible': 925, 'capability': 926, 'passion': 927, 'fascinated': 928, 'evolve': 929, 'railway': 930, 'signals': 931, 'determiner': 932, 'relay': 933, 'weight': 934, 'garodia': 935, 'technosciences': 936, 'jimmy': 937, 'gartner': 938, 'express': 939, 'pricing': 940, 'optimising': 941, 'route': 942, 'possessing': 943, 'accurate': 944, 'guidance': 945, 'architect': 946, 'proficiency': 947, 'video': 948, 'scripture': 949, 'anthony': 950, 'guap': 951, 'latent': 952, 'guided': 953, 'mechanisms': 954, 'vizon': 955, 'vgg16': 956, 'sadar': 957, 'chand': 958, 'mumbai': 959, 'predicting': 960, 'dispute': 961, 'day': 962, 'invoice': 963, 'john': 964, 'allen': 965, 'erp': 966, 'implementation': 967, 'contribution': 968, 'injunction': 969, 'liaison': 970, 'methods': 971, 'environments': 972, 'jvv': 973, 'amelia': 974, 'baker': 975, 'coercive': 976, 'attitude': 977, 'strive': 978, 'competency': 979, 'foot': 980, 'huggingface': 981, 'gru': 982, 'sju': 983, 'karnataka': 984, 'nptel': 985, 'gradute': 986, 'topic': 987, 'arbitration': 988, 'regonition': 989, 'modulated': 990, 'carefully': 991, 'diametric': 992, 'loyola': 993, 'pg': 994, 'daniel': 995, 'lopez': 996, 'fact': 997, 'interaction': 998, 'volvo': 999, 'accord': 1000, 'gujarat': 1001, 'conclude': 1002, 'point': 1003, 'refining': 1004, 'taxi': 1005, 'fare': 1006, 'predictions': 1007, 'default': 1008, 'advisor': 1009, 'employ': 1010, 'usage': 1011, 'tenure': 1012, 'econometrics': 1013, 'solving': 1014, 'scorecard': 1015, 'ifrs': 1016, 'shaheed': 1017, 'bhagat': 1018, 'isi': 1019, 'gold': 1020, 'trend': 1021, 'transactiondetail': 1022, 'anomalous': 1023, 'flaggingfraudulent': 1024, 'wyatt': 1025, 'cooper': 1026, 'deloitte': 1027, 'consulting': 1028, 'private': 1029, '2020to': 1030, 'incorporation': 1031, 'graphml': 1032, 'host': 1033, 'primarily': 1034, 'us': 1035, 'undertake': 1036, 'numerous': 1037, 'initiative': 1038, 'agnostic': 1039, 'aim': 1040, 'accelerate': 1041, 'jkg': 1042, 'nashik': 1043, 'computational': 1044, 'bsi': 1045, 'bombay': 1046, 'audrey': 1047, 'coleson': 1048, 'greatly': 1049, 'transmission': 1050, 'eagerness': 1051, 'exciting': 1052, 'hardware': 1053, 'derived': 1054, 'chunk': 1055, 'on': 1056, 'cummins': 1057, 'microcontroller': 1058, 'access': 1059, 'band': 1060, 'frequency': 1061, 'carry': 1062, 'message': 1063, 'bitrate': 1064, 'gaving': 1065, 'wilson': 1066, 'conjugate': 1067, 'perfect': 1068, 'ideology': 1069, 'kinsgley': 1070, 'co': 1071, 'chadrasekhar': 1072, 'automating': 1073, 'deriving': 1074, 'centric': 1075, 'cis': 1076, 'construct': 1077, 'robotics': 1078, 'dynamic': 1079, 'queries': 1080, 'at': 1081, 'pixel': 1082, 'maskrcnn': 1083, 'clientele': 1084, 'picture': 1085, 'forgery': 1086, 'pes': 1087, 'aw': 1088, 'xml': 1089, 'angularjs': 1090, 'restapi': 1091, 'error': 1092, 'convolution': 1093, 'nolan': 1094, 'clark': 1095, 'instructor': 1096, 'undergraduate': 1097, 'thesis': 1098, 'paper': 1099, 'entrepreneurship': 1100, 'accumulate': 1101, 'nearly': 1102, 'half': 1103, 'attunement': 1104, 'association': 1105, 'root': 1106, 'cause': 1107, 'soil': 1108, 'moisture': 1109, 'cellular': 1110, 'disconfiguration': 1111, 'madelyn': 1112, 'milner': 1113, 'lookout': 1114, 'pillow': 1115, 'brane': 1116, 'jt': 1117, 'kanchipuram': 1118, 'funnel': 1119, 'sure': 1120, 'proof': 1121, 'concepts': 1122, 'snake': 1123, 'breeds': 1124, 'infection': 1125, 'skin': 1126, 'granulation': 1127, 'pyimagesearch': 1128, 'throttle': 1129, 'h2o': 1130, 'hcl': 1131, 'local': 1132, 'dravidian': 1133, 'languages': 1134, 'translate': 1135, 'accurately': 1136, 'english': 1137, 'stc': 1138, 'pondicherry': 1139, 'interpretation': 1140, 'amaze': 1141, 'profiling': 1142, 'gentle': 1143, 'â„¢': 1144, 'featurization': 1145, 'heuristics': 1146, 'proactive': 1147, 'flair': 1148, 'adopt': 1149, 'emerge': 1150, 'organizational': 1151, 'profitability': 1152, 'norm': 1153, 'jp': 1154, 'major': 1155, 'grape': 1156, 'export': 1157, 'dealer': 1158, 'punjabi': 1159, 'patiala': 1160, 'beginner': 1161, 'utilizingdatabase': 1162, 'armed': 1163, 'bug': 1164, 'fix': 1165, 'oop': 1166, 'defect': 1167, 'unit': 1168, 'animal': 1169, 'icpc': 1170, 'reward': 1171, 'elocutioner': 1172, 'titli': 1173, 'specially': 1174, 'housing': 1175, 'stacey': 1176, 'belarny': 1177, 'jiot': 1178, 'belemundi': 1179, 'invictus': 1180, 'qualis': 1181, 'residual': 1182, 'normality': 1183, 'univariate': 1184, 'variate': 1185, 'multi': 1186, 'variance': 1187, 'outlier': 1188, 'treatment': 1189, 'dome': 1190, 'howard': 1191, 'goodman': 1192, 'absolutely': 1193, 'vaguely': 1194, 'zynta': 1195, 'textual': 1196, 'asl': 1197, 'vice': 1198, 'versa': 1199, 'kera': 1200, 'carolin': 1201, 'intricacy': 1202, 'xcelsior': 1203, 'dbscan': 1204, 'number': 1205, 'setup': 1206, 'flutter': 1207, 'kotlin': 1208, 'nit': 1209, 'hamirpur': 1210, 'metallurgy': 1211, 'cit': 1212, 'calicut': 1213, 'jade': 1214, 'roberts': 1215, 'derivative': 1216, 'chennal': 1217, 'sigma': 1218, 'belt': 1219, 'sector': 1220, 'remark': 1221, 'scalle': 1222, 'mason': 1223, 'markinov': 1224, 'standard': 1225, 'catboost': 1226, 'loss': 1227, 'birsimbal': 1228, 'tml': 1229, 'w': 1230, 'r': 1231, 'limit': 1232, 'hmm': 1233, 'bounding': 1234, 'heuristic': 1235, 'hypothesis': 1236, 'introduce': 1237, 'applicant': 1238, 'gather': 1239, 'feel': 1240, 'crison': 1241, 'global': 1242, 'desk': 1243, 'dialogue': 1244, 'summarization': 1245, 'techniques': 1246, 'productintroduce': 1247, 'buildingthem': 1248, 'franchiseowner': 1249, 'ow': 1250, 'deplyoe': 1251, 'net': 1252, 'separate': 1253, 'passenger': 1254, 'distance': 1255, 'k': 1256, 'hs': 1257, 'coalesce': 1258, 'maya': 1259, 'hayden': 1260, 'knack': 1261, 'fill': 1262, 'enthusiasm': 1263, 'dedicated': 1264, 'sota': 1265, 'publication': 1266, 'purpose': 1267, 'captioning': 1268, 'principle': 1269, 'owen': 1270, 'shaw': 1271, 'fallout': 1272, 'orchestration': 1273, 'ssgm': 1274, 'maharashtra': 1275, 'mikayla': 1276, 'neuer': 1277, 'early': 1278, 'change': 1279, 'indosis': 1280, 'care': 1281, 'reduce': 1282, 'cost': 1283, 'ease': 1284, 'maintenance': 1285, 'hoogly': 1286, 'workflow': 1287, 'recent': 1288, 'exercise': 1289, 'update': 1290, 'turnover': 1291, 'tester': 1292, 'dass': 1293, 'rourkela': 1294, 'joseline': 1295, 'hernandez': 1296, 'brainstrom': 1297, 'paragraph': 1298, 'gpt2': 1299, 'ibb': 1300, 'instal': 1301, 'inside': 1302, 'skn': 1303, 'coimbatore': 1304, 'preprocessing': 1305, 'glove': 1306, 'lucene': 1307, 'devops': 1308, 'spacy': 1309, 'stanford': 1310, 'affect': 1311, 'selection': 1312, 'damian': 1313, 'rossier': 1314, 'modeler': 1315, 'depend': 1316, 'aid': 1317, 'finally': 1318, 'jr': 1319, 'reliability': 1320, 'repairable': 1321, 'tractor': 1322, 'parts': 1323, 'formulas': 1324, 'functions': 1325, 'cognibot': 1326, 'robot': 1327, 'integrated': 1328, 'rely': 1329, 'skillset': 1330, 'medicine': 1331, 'databricks': 1332, 'factory': 1333, 'amass': 1334, 'great': 1335, 'full': 1336, 'desarks': 1337, 'prm': 1338, 'lifecycle': 1339, 'thefraud': 1340, 'analyse': 1341, 'plot': 1342, 'someinsight': 1343, 'graphic': 1344, 'tomap': 1345, 'simplify': 1346, 'michael': 1347, 'reever': 1348, 'systemica': 1349, 'consultants': 1350, 'effectively': 1351, 'progress': 1352, 'manipulating': 1353, 'dimensionality': 1354, 'vary': 1355, 'enhanced': 1356, 'procedure': 1357, 'relevant': 1358, 'visual': 1359, 'studio': 1360, 'unix': 1361, 'entire': 1362, 'ids': 1363, 'suitable': 1364, 'actionable': 1365, 'skille': 1366, 'direction': 1367, 'fuel': 1368, 'capable': 1369, 'decisionmake': 1370, 'stakeholder': 1371, 'steering': 1372, 'option': 1373, 'survey': 1374, 'food': 1375, 'recommending': 1376, 'programmer': 1377, 'macros': 1378, 'setting': 1379, 'template': 1380, 'deliverable': 1381, 'feed': 1382, 'pollution': 1383, 'nist': 1384, 'kottam': 1385, 'nyugen': 1386, 'ra': 1387, 'aifferent': 1388, 'interpreting': 1389, 'effective': 1390, 'banhadau': 1391, 'translation': 1392, 'hsbc': 1393, 'intermittent': 1394, 'fiat': 1395, 'currency': 1396, 'stabilitie': 1397, 'dbb': 1398, 'himachal': 1399, 'exploratory': 1400, 'indicator': 1401, 'forecast': 1402, 'agricultural': 1403, 'gdp': 1404, 'quarter': 1405, 'haylee': 1406, 'rogers': 1407, 'digiscape': 1408, 'exam': 1409, 'vector': 1410, 'raster': 1411, 'sagemaker': 1412, 'lustre': 1413, 'fsx': 1414, 'node': 1415, 'processor': 1416, 'cohesive': 1417, 'ridge': 1418, 'lasso': 1419, 'cool': 1420, 'meter': 1421, 'crowd': 1422, 'tag': 1423, 'remiro': 1424, 'amio': 1425, 'culture': 1426, 'shock': 1427, 'djr': 1428, 'calculate': 1429, 'sentence': 1430, 'main': 1431, 'cleansing': 1432, 'verifying': 1433, 'integrity': 1434, 'etl': 1435, 'record': 1436, 'alignment': 1437, 'expectation': 1438, 'abnormal': 1439, 'empower': 1440, 'bachelor': 1441, 'utility': 1442, 'remove': 1443, 'noiseuse': 1444, 'fourier': 1445, 'function': 1446, 'ashley': 1447, 'haydon': 1448, 'period': 1449, 'choose': 1450, 'star': 1451, 'magicpin': 1452, 'creation': 1453, 'scripts': 1454, 'cron': 1455, 'jeco': 1456, 'bethany': 1457, 'cummin': 1458, 'kpg': 1459, 'march': 1460, 'ict': 1461, 'summer': 1462, 'administration': 1463, 'cloudmail': 1464, 'block': 1465, 'beanstalk': 1466, 'load': 1467, 'simple': 1468, 'glacier': 1469, 'etc': 1470, 'activitie': 1471, 'sharpe': 1472, 'biomedical': 1473, 'combine': 1474, 'kpi': 1475, 'direct': 1476, 'projeect': 1477, 'adobe': 1478, 'blending': 1479, 'uniform': 1480, 'abul': 1481, 'bu': 1482, 'external': 1483, 'tanisha': 1484, 'quromba': 1485, 'brokerage': 1486, 'units': 1487, 'kashwant': 1488, 'general': 1489, 'resource': 1490, 'supply': 1491, 'silver': 1492, 'medal': 1493, 'finalist': 1494, 'pan': 1495, 'winner': 1496, 'economist': 1497, 'respectively': 1498, 'scala': 1499, 'specialize': 1500, 'prove': 1501, 'supervision': 1502, 'guide': 1503, 'song': 1504, 'title': 1505, 'quantimum': 1506, 'nazapp': 1507, 'deeper': 1508, 'nyu': 1509, 'preparation': 1510, 'customeranalysis': 1511, 'applicationsdevelope': 1512, 'indentify': 1513, 'fakewebsite': 1514, 'andnlp': 1515, 'jose': 1516, 'garcia': 1517, 'cb': 1518, 'ledger': 1519, 'helena': 1520, 'patricks': 1521, 'chrysalis': 1522, 'inclusive': 1523, 'miet': 1524, 'baliya': 1525, 'up': 1526, 'pytesseract': 1527, 'means': 1528, 'united': 1529, 'health': 1530, 'revolve': 1531, 'criteria': 1532, 'automatic': 1533, 'mapper': 1534, 'matching': 1535, 'caltech': 1536, 'sydney': 1537, 'jones': 1538, 'jira': 1539, 'mlops': 1540, 'tsc': 1541, 'electronic': 1542, 'ste': 1543, 'automotive': 1544, 'manufacture': 1545, 'fault': 1546, 'photo': 1547, 'basher': 1548, 'correspondent': 1549, 'bib': 1550, 'cat': 1551, 'rms': 1552, 'maths': 1553, '2015': 1554, 'quantify': 1555, 'catastropherisk': 1556, 'conceptsto': 1557, 'account': 1558, 'variousinsurance': 1559, 'reinsurance': 1560, 's2h': 1561, 'minds': 1562, 'osmania': 1563, 'clouds': 1564, 'interactive': 1565, 'quiz': 1566, 'underneath': 1567, 'waterloo': 1568, 'aspen': 1569, 'minitab': 1570, 'catalysis': 1571, 'composition': 1572, 'organic': 1573, 'molecular': 1574, 'telemetry': 1575, 'boundary': 1576, 'meticulous': 1577, 'reaction': 1578, 'involved': 1579, 'looking': 1580, 'sk': 1581, 'jadodia': 1582, 'asistant': 1583, 'checmical': 1584, 'steamconsumption': 1585, 'msfe': 1586, 'using': 1587, 'paint': 1588, 'molecule': 1589, 'affectedbreathe': 1590, 'algorithmic': 1591, 'beadde': 1592, 'industrial': 1593, 'reactionwithout': 1594, 'toxic': 1595, 'waste': 1596, 'sanrachna': 1597, 'sgt': 1598, 'deputy': 1599, 'secondary': 1600, 'particularly': 1601, 'weather': 1602, 'rshiny': 1603, 'quantitative': 1604, 'animate': 1605, 'graphical': 1606, 'representation': 1607, 'wang': 1608, 'dkb': 1609, 'innovations': 1610, 'shopping': 1611, 'site': 1612, 'heramba': 1613, 'chandra': 1614, 'pycharm': 1615, 'pierre': 1616, 'franks': 1617, 'informative': 1618, 'sense': 1619, 'convey': 1620, 'exhibited': 1621, 'timeliness': 1622, 'proposals': 1623, 'rfi': 1624, 'rfq': 1625, 'creative': 1626, 'crisp': 1627, 'proposal': 1628, 'coordinate': 1629, 'sme': 1630, 'stakeholders': 1631, 'plan': 1632, 'sppu': 1633, 'amber': 1634, 'show': 1635, 'responsibilitie': 1636, 'zoombot': 1637, 'cognitive': 1638, 'division': 1639, 'based': 1640, 'supervise': 1641, 'hem': 1642, 'indore': 1643, 'bill': 1644, 'clifford': 1645, 'alteryx': 1646, 'orchidia': 1647, 'tours': 1648, 'malviya': 1649, 'ahmedabad': 1650, 'prioritization': 1651, 'decline': 1652, 'incorporate': 1653, 'entertainment': 1654, 'registration': 1655, 'taylor': 1656, 'benley': 1657, 'biswa': 1658, 'sample': 1659, 'jjr': 1660, 'uttaranchal': 1661, 'nodejs': 1662, 'ellie': 1663, 'mackey': 1664, 'bb': 1665, 'developers': 1666, 'learned': 1667, 'subhash': 1668, 'eagerly': 1669, 'jayla': 1670, 'ramirez': 1671, 'geakminds': 1672, 'material': 1673, 'plant': 1674, 'optimum': 1675, 'inventory': 1676, 'productivity': 1677, 'analyzed': 1678, 'utilization': 1679, 'gateway': 1680, 'deploying': 1681, 'present': 1682, 'lambda': 1683, 'geo': 1684, 'diasters': 1685, 'nano': 1686, 'udacity': 1687, 'com': 1688, 'madras': 1689, 'srmu': 1690, 'avid': 1691, 'step': 1692, 'horizon': 1693, 'bigquery': 1694, 'kibana': 1695, 'polymon': 1696, 'cap': 1697, 'jain': 1698, 'jumpstart': 1699, 'july': 1700, 'darjeeling': 1701, 'calcutta': 1702, 'pig': 1703, 'sklearn': 1704, 'classify': 1705, 'cotton': 1706, 'leaf': 1707, 'legal': 1708, 'informed': 1709, 'experiment': 1710, 'promise': 1711, 'automated': 1712, 'telco': 1713, 'bio': 1714, 'lisa': 1715, 'jennings': 1716, 'hard': 1717, 'sahara': 1718, 'medica': 1719, 'visveswaraiah': 1720, 'vtu': 1721, 'incident': 1722, 'raised': 1723, 'lot': 1724, 'axis': 1725, 'csi': 1726, 'psi': 1727, 'assess': 1728, 'birla': 1729, 'bit': 1730, 'unified': 1731, 'interface': 1732, 'upi': 1733, 'topredict': 1734, 'ntb': 1735, 'ktb': 1736, 'known': 1737, 'approve': 1738, 'card': 1739, 'byutilize': 1740, 'ingest': 1741, 'andprocess': 1742, 'nicholas': 1743, 'mendes': 1744, 'enlisting': 1745, 'categorization': 1746, 'bootstrap': 1747, 'jdbc': 1748, 'hib': 1749, 'drew': 1750, 'hall': 1751, 'retrain': 1752, 'prototype': 1753, 'frameworks': 1754, 'dl': 1755, 'bristlecone': 1756, 'populate': 1757, 'seamless': 1758, 'thakur': 1759, 'shiv': 1760, 'kumar': 1761, 'diploma': 1762, 's': 1763, 'devnagari': 1764, 'ltt': 1765, 'onwebpage': 1766, 'naxapp': 1767, 'fake': 1768, 'enjoy': 1769, 'utilise': 1770, 'collaborative': 1771, 'advice': 1772, 'jupyter': 1773, 'blood': 1774, 'importing': 1775, 'shawn': 1776, 'buffet': 1777, 'saiko': 1778, 'august': 1779, 'roles': 1780, 'libraries': 1781, 'artistry': 1782, 'gesture': 1783, 'controlled': 1784, 'robotic': 1785, 'lawn': 1786, 'mower': 1787, 'sl': 1788, 'thala': 1789, 'orissa': 1790, 'arsenal': 1791, 'coordinative': 1792, 'kubeflow': 1793, 'browse': 1794, 'rto': 1795, 'origin': 1796, 'one97': 1797, 'communications': 1798, 'redact': 1799, 'yolo': 1800, 'cnn': 1801, 'drop': 1802, 'possible': 1803, 'sub': 1804, 'space': 1805, 'claim': 1806, 'probability': 1807, 'estimate': 1808, 'premium': 1809, 'charge': 1810, 'sgbau': 1811, 'kanpur': 1812, 'stella': 1813, 'thatcher': 1814, 'kripa': 1815, 'idea': 1816, 'perceive': 1817, 'unstructered': 1818, 'mechanical': 1819, 'bhavleesh': 1820, 'duggar': 1821, 'nasik': 1822, 'grasp': 1823, 'stock': 1824, 'kelsey': 1825, 'stens': 1826, 'klp': 1827, 'built': 1828, 'dr': 1829, 'jagjiban': 1830, 'rao': 1831, 'laravel': 1832, 'integrate': 1833, 'hmi': 1834, 'head': 1835, 'ondevice': 1836, 'ux': 1837, 'gruvil': 1838, 'scrap': 1839, 'structured': 1840, 'format': 1841, 'drdo': 1842, 'identifier': 1843, 'satellite': 1844, 'capture': 1845, 'coordination': 1846, 'panda': 1847, 'mask': 1848, 'benjamin': 1849, 'osta': 1850, 'flipkart': 1851, 'compression': 1852, 'anna': 1853, 'robust': 1854, 'mapreduce': 1855, 'hdfs': 1856, 'yarn': 1857, 'informatica': 1858, 'talend': 1859, 'redshift': 1860, 'coca': 1861, 'cola': 1862, 'shipment': 1863, 'warehousing': 1864, 'immediately': 1865, 'batch': 1866, 'aset': 1867, 'jaroslav': 1868, 'chechnik': 1869, 'interact': 1870, 'benefit': 1871, 'anaconda': 1872, 'streamlit': 1873, 'raspberry': 1874, 'pi': 1875, 'demonstrate': 1876, 'energy': 1877, 'resources': 1878, 'sadhana': 1879, 'insights': 1880, 'answering': 1881, 'unstructureddata': 1882, 'dataacquisition': 1883, 'topicmodelle': 1884, 'mirad': 1885, 'yastein': 1886, 'assignment': 1887, 'larsen': 1888, 'toubro': 1889, 'kancheepuram': 1890, 'chennai': 1891, 'wesbite': 1892, 'fully': 1893, 'customizable': 1894, 'ksst': 1895, 'scholar': 1896}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "train_sequences = tokenizer.texts_to_sequences(filtered_resume_list_train)\r\n",
    "train_padded = pad_sequences(train_sequences, padding=padding_type, maxlen=max_length)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "train_padded[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([810, 811,  55,  26,   8,  93, 535, 184,  99,   2,  22,   4,   3,\n",
       "        23, 812, 813, 235,  15, 277,  61, 323,   9, 120, 121, 100, 536,\n",
       "        15, 537, 236, 538,  32,   7,  84, 132,  75, 237, 539,  34,  24,\n",
       "       206,  94,  35,  30,  12,  13, 278, 324, 402,  21,  10,   6,  43,\n",
       "       185,  62, 186, 187, 133, 109, 162, 122,   4,   3,  73, 814, 403,\n",
       "       815,  56, 238, 816, 122,  14, 404, 540, 101, 207, 541, 542,  94,\n",
       "       102,  18,   3, 543,   4,   3,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0])"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "validation_sequences = tokenizer.texts_to_sequences(filtered_resume_list_test)\r\n",
    "validation_padded = pad_sequences(validation_sequences, padding=padding_type, maxlen=max_length)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "validation_padded[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ 57, 114,   2,  23,  47, 371,   1,   9,  98,   9, 120, 128, 345,\n",
       "         1,  55,  35,   4,   3,   2,  19,  14,  42,  27,  37, 715, 511,\n",
       "        15, 410,  70,  88,  70,  88,   8,  17, 530,   1, 113,  10,  85,\n",
       "         1,  64,  15,   1, 217,   9, 188,   1,  47, 128, 266, 190, 188,\n",
       "       165, 365,  14, 424, 389, 246,   1,   1,  37,   1,  27,  36,  22,\n",
       "         1, 119,  49, 305,   7, 168,  27,  36,  22,  14,   1,   1, 535,\n",
       "         1,  37,   1,  74,  74, 677,  14, 115,  26,   1, 145,  65,  12,\n",
       "        13, 348,  38,  49,   0,   0,   0,   0,   0])"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "train_padded_df = pd.DataFrame(train_padded)\r\n",
    "test_padded_df = pd.DataFrame(validation_padded)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "train_df = pd.concat([train_df, train_padded_df], axis = 1)\r\n",
    "test_df = pd.concat([test_df, test_padded_df], axis = 1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "import re"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "# train features\r\n",
    "train_df['words_counts'] = train_df['resumes'].apply(lambda x: len(str(x).split()))\r\n",
    "train_df['char_counts'] = train_df['resumes'].apply(lambda x: len(str(x)))\r\n",
    "train_df['avg_word_len'] = train_df['char_counts']/train_df['words_counts']\r\n",
    "train_df['ml_counts'] = train_df['resumes'].apply(lambda x: len(re.findall('machine learning', x)))\r\n",
    "train_df['ml_engineer_counts'] = train_df['resumes'].apply(lambda x: len(re.findall('machine learning engineer', x)))\r\n",
    "train_df['analytics_counts'] = train_df['resumes'].apply(lambda x: len(re.findall('analytics', x)))\r\n",
    "train_df['degree_counts'] = train_df['resumes'].apply(lambda x: len(re.findall('master degree', x)))\r\n",
    "train_df['degree_counts_2'] = train_df['resumes'].apply(lambda x: len(re.findall('msc', x)))\r\n",
    "train_df['degree_counts_3'] = train_df['resumes'].apply(lambda x: len(re.findall('degree', x)))\r\n",
    "train_df['deep_learning_counts'] = train_df['resumes'].apply(lambda x: len(re.findall('deep learning', x)))\r\n",
    "train_df['tf_counts'] = train_df['resumes'].apply(lambda x: len(re.findall('tensorflow', x)))\r\n",
    "train_df['neural_network_counts'] = train_df['resumes'].apply(lambda x: len(re.findall('neural network', x)))\r\n",
    "train_df['nlp_counts'] = train_df['resumes'].apply(lambda x: len(re.findall('natural language processing', x)))\r\n",
    "train_df['nlp_counts_2'] = train_df['resumes'].apply(lambda x: len(re.findall('nlp', x)))\r\n",
    "train_df['pyspark_counts'] = train_df['resumes'].apply(lambda x: len(re.findall('pyspark', x)))\r\n",
    "train_df['hadoop_counts'] = train_df['resumes'].apply(lambda x: len(re.findall('hadoop', x)))\r\n",
    "train_df['data_analysis_counts'] = train_df['resumes'].apply(lambda x: len(re.findall('data analysis', x)))\r\n",
    "train_df['lustering_counts'] = train_df['resumes'].apply(lambda x: len(re.findall('clustering', x)))\r\n",
    "train_df['lr_counts'] = train_df['resumes'].apply(lambda x: len(re.findall('logistic regression', x)))\r\n",
    "train_df['classification_counts'] = train_df['resumes'].apply(lambda x: len(re.findall('classification', x)))\r\n",
    "train_df['sk_counts'] = train_df['resumes'].apply(lambda x: len(re.findall('sciKit learn', x)))\r\n",
    "train_df['pytorch_counts'] = train_df['resumes'].apply(lambda x: len(re.findall('pytorch', x)))\r\n",
    "train_df['cnn_counts'] = train_df['resumes'].apply(lambda x: len(re.findall('cnn', x)))\r\n",
    "train_df['rnn_counts'] = train_df['resumes'].apply(lambda x: len(re.findall('rnn', x)))\r\n",
    "train_df['gans_counts'] = train_df['resumes'].apply(lambda x: len(re.findall('gans', x)))\r\n",
    "train_df['nltk_counts'] = train_df['resumes'].apply(lambda x: len(re.findall('nltk', x)))\r\n",
    "train_df['spacy_counts'] = train_df['resumes'].apply(lambda x: len(re.findall('spacy', x)))\r\n",
    "train_df['transformer_counts'] = train_df['resumes'].apply(lambda x: len(re.findall('transformer', x)))\r\n",
    "train_df['django_counts'] = train_df['resumes'].apply(lambda x: len(re.findall('django', x)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "# test features\r\n",
    "test_df['words_counts'] = test_df['resumes'].apply(lambda x: len(str(x).split()))\r\n",
    "test_df['char_counts'] = test_df['resumes'].apply(lambda x: len(str(x)))\r\n",
    "test_df['avg_word_len'] = test_df['char_counts']/train_df['words_counts']\r\n",
    "test_df['ml_counts'] = test_df['resumes'].apply(lambda x: len(re.findall('machine learning', x)))\r\n",
    "test_df['ml_engineer_counts'] = test_df['resumes'].apply(lambda x: len(re.findall('machine learning engineer', x)))\r\n",
    "test_df['analytics_counts'] = test_df['resumes'].apply(lambda x: len(re.findall('analytics', x)))\r\n",
    "test_df['degree_counts'] = test_df['resumes'].apply(lambda x: len(re.findall('master degree', x)))\r\n",
    "test_df['degree_counts_2'] = test_df['resumes'].apply(lambda x: len(re.findall('msc', x)))\r\n",
    "test_df['degree_counts_3'] = test_df['resumes'].apply(lambda x: len(re.findall('degree', x)))\r\n",
    "test_df['deep_learning_counts'] = test_df['resumes'].apply(lambda x: len(re.findall('deep learning', x)))\r\n",
    "test_df['tf_counts'] = test_df['resumes'].apply(lambda x: len(re.findall('tensorflow', x)))\r\n",
    "test_df['neural_network_counts'] = test_df['resumes'].apply(lambda x: len(re.findall('neural network', x)))\r\n",
    "test_df['nlp_counts'] = test_df['resumes'].apply(lambda x: len(re.findall('natural language processing', x)))\r\n",
    "test_df['nlp_counts_2'] = test_df['resumes'].apply(lambda x: len(re.findall('nlp', x)))\r\n",
    "test_df['pyspark_counts'] = test_df['resumes'].apply(lambda x: len(re.findall('pyspark', x)))\r\n",
    "test_df['hadoop_counts'] = test_df['resumes'].apply(lambda x: len(re.findall('hadoop', x)))\r\n",
    "test_df['data_analysis_counts'] = test_df['resumes'].apply(lambda x: len(re.findall('data analysis', x)))\r\n",
    "test_df['lustering_counts'] = test_df['resumes'].apply(lambda x: len(re.findall('clustering', x)))\r\n",
    "test_df['lr_counts'] = test_df['resumes'].apply(lambda x: len(re.findall('logistic regression', x)))\r\n",
    "test_df['classification_counts'] = test_df['resumes'].apply(lambda x: len(re.findall('classification', x)))\r\n",
    "test_df['sk_counts'] = test_df['resumes'].apply(lambda x: len(re.findall('sciKit learn', x)))\r\n",
    "test_df['pytorch_counts'] = test_df['resumes'].apply(lambda x: len(re.findall('pytorch', x)))\r\n",
    "test_df['cnn_counts'] = test_df['resumes'].apply(lambda x: len(re.findall('cnn', x)))\r\n",
    "test_df['rnn_counts'] = test_df['resumes'].apply(lambda x: len(re.findall('rnn', x)))\r\n",
    "test_df['gans_counts'] = test_df['resumes'].apply(lambda x: len(re.findall('gans', x)))\r\n",
    "test_df['nltk_counts'] = test_df['resumes'].apply(lambda x: len(re.findall('nltk', x)))\r\n",
    "test_df['spacy_counts'] = test_df['resumes'].apply(lambda x: len(re.findall('spacy', x)))\r\n",
    "test_df['transformer_counts'] = test_df['resumes'].apply(lambda x: len(re.findall('transformer', x)))\r\n",
    "test_df['django_counts'] = test_df['resumes'].apply(lambda x: len(re.findall('django', x)))\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "from sklearn.feature_selection import mutual_info_regression\r\n",
    "def make_mi_scores(X, y):\r\n",
    "    mi_scores = mutual_info_regression(X, y)\r\n",
    "    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\r\n",
    "    mi_scores = mi_scores.sort_values(ascending=False)\r\n",
    "    return round(mi_scores, 3)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "print(train_df.head(5))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   Match Percentage                                            resumes    0  \\\n",
      "0             13.60  jacob smith personal profile work background a...  810   \n",
      "1             36.63  brianna williams executive profile work experi...  545   \n",
      "2             54.93  associate analyst skills certified data analys...  111   \n",
      "3             41.46  python machine learn deep learning data analys...    6   \n",
      "4             48.91  jennifer armstrong fresher computer vision mac...  857   \n",
      "\n",
      "     1   2    3    4    5    6    7  ...  classification_counts  sk_counts  \\\n",
      "0  811  55   26    8   93  535  184  ...                      1          0   \n",
      "1  407  14   30  326  239  546   21  ...                      0          0   \n",
      "2   22  28  412    2   22  210  150  ...                      0          0   \n",
      "3    4  11   18    3    2    5   45  ...                      0          0   \n",
      "4  858  58   45   79    4    3   23  ...                      0          0   \n",
      "\n",
      "   pytorch_counts  cnn_counts  rnn_counts  gans_counts  nltk_counts  \\\n",
      "0               0           0           0            0            0   \n",
      "1               0           0           0            0            0   \n",
      "2               0           0           0            0            0   \n",
      "3               0           0           0            0            0   \n",
      "4               0           0           0            0            0   \n",
      "\n",
      "   spacy_counts  transformer_counts  django_counts  \n",
      "0             0                   0              0  \n",
      "1             0                   0              0  \n",
      "2             0                   0              0  \n",
      "3             0                   0              0  \n",
      "4             0                   0              0  \n",
      "\n",
      "[5 rows x 131 columns]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "train_df.shape, test_df.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((90, 131), (60, 130))"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "train_df.drop('resumes', axis = 1, inplace=True)\r\n",
    "test_df.drop('resumes', axis = 1, inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "x = train_df.drop('Match Percentage', axis = 1)\r\n",
    "y = train_df['Match Percentage']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "#Feature scaling\r\n",
    "from sklearn.preprocessing import StandardScaler\r\n",
    "scale = StandardScaler()\r\n",
    "x = scale.fit_transform(x)\r\n",
    "test_df = scale.transform(test_df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "x"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[ 2.41049604,  2.79696195, -0.39026728, ..., -0.10599979,\n",
       "        -0.21566555, -0.24253563],\n",
       "       [ 1.44686328,  1.18366283, -0.56807766, ..., -0.10599979,\n",
       "        -0.21566555, -0.24253563],\n",
       "       [-0.13131264, -0.35376331, -0.50736192, ..., -0.10599979,\n",
       "        -0.21566555, -0.24253563],\n",
       "       ...,\n",
       "       [-0.53131115, -0.43762292, -0.3815936 , ..., -0.10599979,\n",
       "        -0.21566555, -0.24253563],\n",
       "       [ 0.17050441, -0.43762292, -0.59409869, ..., -0.10599979,\n",
       "         4.63680925, -0.24253563],\n",
       "       [-0.53131115, -0.43762292, -0.39026728, ..., -0.10599979,\n",
       "        -0.21566555, -0.24253563]])"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mi_score1 = make_mi_scores(x, y)\r\n",
    "print(mi_score1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# feature selection\r\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_regression\r\n",
    "selector = SelectKBest(f_regression, k=100)\r\n",
    "X = selector.fit_transform(x, y)\r\n",
    "test_df = selector.transform(test_df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from keras.models import Sequential\r\n",
    "from keras.layers import Dense, Activation, Flatten\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.ensemble import RandomForestRegressor\r\n",
    "from sklearn.metrics import mean_absolute_error \r\n",
    "from matplotlib import pyplot as plt\r\n",
    "import seaborn as sb\r\n",
    "import matplotlib.pyplot as plt\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_df.shape[1]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "NN_model = Sequential()\r\n",
    "\r\n",
    "# The Input Layer :\r\n",
    "NN_model.add(Dense(128, kernel_initializer='normal', activation='relu'))\r\n",
    "\r\n",
    "# The Hidden Layers :\r\n",
    "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\r\n",
    "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\r\n",
    "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\r\n",
    "\r\n",
    "# The Output Layer :\r\n",
    "NN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Compile the network :\r\n",
    "NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "NN_model.fit(x, y, epochs=100, batch_size=32)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "from xgboost import XGBRegressor\r\n",
    "XGBModel = XGBRegressor()\r\n",
    "XGBModel.fit(x,y , verbose=False)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "             importance_type='gain', interaction_constraints='',\n",
       "             learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "             n_estimators=100, n_jobs=8, num_parallel_tree=1, random_state=0,\n",
       "             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
       "             tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "def submission(model, test):\r\n",
    "    test1 = pd.read_csv('dataset/test.csv')\r\n",
    "    preds = model.predict(test)\r\n",
    "    prediction = pd.DataFrame(preds, columns = ['Match Percentage'])\r\n",
    "    sub_df = pd.concat([test1, prediction], axis = 1)\r\n",
    "    return sub_df\r\n",
    "\r\n",
    "sub = submission(XGBModel, test_df)\r\n",
    "sub.to_csv('submission file/Submission-17.csv')\r\n",
    "print(sub.head())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "     CandidateID  Match Percentage\n",
      "0  candidate_014         44.282951\n",
      "1  candidate_098         47.256233\n",
      "2  candidate_075         24.159756\n",
      "3  candidate_016         28.871172\n",
      "4  candidate_131         22.501472\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "interpreter": {
   "hash": "0bd2b4fb9f3446da4d8f55e0ab2f46c8924547cca4f6669133edbde87e094ed0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}